title: 【ES】集群架构
author: haifun
tags:
  - Elasticsearch
categories:
  - 搜索引擎
date: 2023-03-02 18:00:00

---

# 节点类型

ES 集群中可以有多种节点，节点类型可以配置：

```yml
node.master: trur/false # 是否是候选主节点
node.data: true/false # 是否是数据节点
```

- 候选主节点：参与选主投票
- 主节点：需是候选主节点，负责索引添加和删除，维护元数据信息（集群节点、索引、索引映射和配置、索引分片和节点对应关系、分片 in-sync 副本集合）
- 数据节点：负责对数据增、删、改、查、聚合等操作，以及数据存储
- 协调节点：任意节点都可作为协调节点接受客户端请求，负责分发请求、收集结果等操作

# 选举master

在 ES 节点启动时，会通过 ping 其他节点得到活跃 master 列表以及候选主节点列表，进而决定是接受已有活跃 master，还是发起 master 选举。

ES 通过指定 `discovery.zen.minimum_master_nodes` 设置 quorum 数量，一般设置为候选主节点数的一半 + 1，使用多数派原则保证只有一个 master。

选主的发起由候选主节点发起，如果当前候选主节点发现自己不是 master 节点，并且通过 ping 其他节点发现无法联系到主节点，并且包括自己在内已经有超过 quorum 个节点无法联系到主节点，那么这个时候则发起选主。

选主时按照集群节点的参数 `<stateVersion, id>` 排序。stateVersion 从大到小排序，id 从小到大排序，选出版本最新、id 最小的候选主节点作为 master（Bully算法）。

# 脑裂问题

在 master-slave 架构中，不可避免的会提到脑裂问题，即集群中选举出多个 master 的情况。

ES 中可能造成脑裂问题原因如下：

- 网络问题：集群间网络出现延迟或者形成网络分区
- 节点假死：主节点既是 mater 又是 data，搜索负载过大/长时间GC 导致失去响应

针对脑裂问题可作出优化措施如下：
1. 适当调大响应超时时间，减少误判。
2. 设置选举时需要参与选举的候选主节点的节点数（`discovery.zen.munimum_master_nodes` 默认1），官方建议值：候选主节点个数/2+1，这样做既能防止脑裂现象的发生，也能最大限度地提升集群的高可用性。
3. 角色分离，将候选主节点与数据节点进行角色分离，减轻主节点负担。

# 元数据同步

当选举出 master 后，接着会同步集群的元数据（node、索引、shard等信息），从而保证数据是最新的。

所有 node 会把自己的集群元数据发送给 master，当 quorum 以上数量的 node 上报集群元数据后，master 会选择一个最新的版本，然后下发给所有 node。当 quorum 以上数量的 node 完成集群元数据同步后，即认为集群元数据整体同步操作成功。

# 数据恢复

master 基于集群元数据会给每个索引分配主副 shard 以及所在的 node，然后下发到 node。

主 shard 所在 node 读取 lucene 磁盘文件以及 translog 文件恢复出索引 shard 数据，然后生成快照同步到副 shard。

# 集群状态

- yellow，如果一个索引主分片分配和恢复完成，索引就是 yellow 状态，所有索引都是 yellow 状态，集群就是 yellow 状态。
- green，如果一个索引所有分片都分配和恢复完成，索引就是 green 状态，所有索引都是 green 状态，集群就是 green 状态。
